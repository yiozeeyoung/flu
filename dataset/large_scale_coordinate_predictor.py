import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool, GATConv, TransformerConv
from torch_geometric.data import DataLoader, InMemoryDataset
import numpy as np
import os
from pathlib import Path
import time
import json
import matplotlib.pyplot as plt
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedCoordinatePredictor(nn.Module):
    """é«˜çº§3Dåæ ‡é¢„æµ‹æ¨¡å‹ - ç”¨äºå¤§è§„æ¨¡è®­ç»ƒ"""
    def __init__(self, input_dim=1, hidden_dim=128, output_dim=3, num_layers=6, model_type='gat'):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.model_type = model_type
        
        # åŸå­åµŒå…¥å±‚
        self.atom_embedding = nn.Embedding(100, hidden_dim)  # æ”¯æŒåŸå­åºæ•°åˆ°100
        
        # å›¾ç¥ç»ç½‘ç»œå±‚
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        
        for i in range(num_layers):
            if model_type == 'gcn':
                conv = GCNConv(hidden_dim, hidden_dim)
            elif model_type == 'gat':
                conv = GATConv(hidden_dim, hidden_dim // 8, heads=8, dropout=0.1)
            elif model_type == 'transformer':
                conv = TransformerConv(hidden_dim, hidden_dim, heads=8, dropout=0.1)
            else:
                conv = GCNConv(hidden_dim, hidden_dim)
            
            self.convs.append(conv)
            self.norms.append(nn.LayerNorm(hidden_dim))
        
        # åæ ‡é¢„æµ‹å¤´
        self.coord_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        
        # åŸå­åµŒå…¥
        if x.dim() == 1:
            x = x.unsqueeze(-1)
        x = self.atom_embedding(x.long().squeeze(-1))
        
        # æ®‹å·®è¿æ¥çš„å›¾å·ç§¯
        for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):
            x_residual = x
            x = conv(x, edge_index)
            x = norm(x)
            x = F.relu(x)
            
            # æ®‹å·®è¿æ¥ï¼ˆç¬¬ä¸€å±‚åå¼€å§‹ï¼‰
            if i > 0:
                x = x + x_residual
            
            x = self.dropout(x)
        
        # é¢„æµ‹åæ ‡
        pred_pos = self.coord_predictor(x)
        
        return pred_pos

class LoadedDataset(InMemoryDataset):
    def __init__(self, root):
        super().__init__(root)
        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)
    
    @property
    def processed_file_names(self):
        return ['data.pt']

class LargeScaleTrainer:
    """å¤§è§„æ¨¡è®­ç»ƒå™¨"""
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(exist_ok=True)
        
        # è®­ç»ƒè®°å½•
        self.train_history = {
            'train_loss': [],
            'val_rmse': [],
            'val_mae': [],
            'learning_rates': []
        }
        
    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        logger.info("åŠ è½½æ•°æ®é›†...")
        data_dir = Path(self.config['data_dir'])
        
        if not (data_dir / "processed" / "data.pt").exists():
            logger.error(f"æ•°æ®é›†æœªæ‰¾åˆ°: {data_dir}")
            raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œæ•°æ®è½¬æ¢è„šæœ¬ç”Ÿæˆ {data_dir}/processed/data.pt")
        
        dataset = LoadedDataset(data_dir)
        logger.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªåˆ†å­")
        
        # é™åˆ¶æ•°æ®å¤§å°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if self.config.get('max_samples') and self.config['max_samples'] < len(dataset):
            dataset = dataset[:self.config['max_samples']]
            logger.info(f"ä½¿ç”¨æ•°æ®: {len(dataset)} ä¸ªåˆ†å­")
        
        return dataset
    
    def normalize_coordinates(self, dataset):
        """åæ ‡å½’ä¸€åŒ–"""
        logger.info("è®¡ç®—åæ ‡å½’ä¸€åŒ–å‚æ•°...")
        
        # é‡‡æ ·ä¸€éƒ¨åˆ†æ•°æ®è®¡ç®—ç»Ÿè®¡é‡ï¼ˆé¿å…å†…å­˜ä¸è¶³ï¼‰
        sample_size = min(10000, len(dataset))
        sample_indices = np.random.choice(len(dataset), sample_size, replace=False)
        
        all_pos = []
        for idx in sample_indices:
            all_pos.append(dataset[idx].pos)
        
        all_pos = torch.cat(all_pos, dim=0)
        pos_mean = all_pos.mean(dim=0)
        pos_std = all_pos.std(dim=0)
        
        logger.info(f"ä½ç½®å‡å€¼: {pos_mean}")
        logger.info(f"ä½ç½®æ ‡å‡†å·®: {pos_std}")
        
        # é˜²æ­¢é™¤é›¶
        pos_std = torch.where(pos_std < 1e-6, torch.ones_like(pos_std), pos_std)
        
        return pos_mean, pos_std
    
    def create_data_loaders(self, dataset, pos_mean, pos_std):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        logger.info("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        
        # æ•°æ®é›†åˆ†å‰²
        total_size = len(dataset)
        train_size = int(0.8 * total_size)
        val_size = int(0.1 * total_size)
        test_size = total_size - train_size - val_size
        
        # éšæœºæ‰“ä¹±
        indices = torch.randperm(total_size)
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        test_indices = indices[train_size + val_size:]
        
        train_dataset = [dataset[i] for i in train_indices]
        val_dataset = [dataset[i] for i in val_indices]
        test_dataset = [dataset[i] for i in test_indices]
        
        logger.info(f"è®­ç»ƒé›†: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†: {len(val_dataset)}")
        logger.info(f"æµ‹è¯•é›†: {len(test_dataset)}")
        
        # å½’ä¸€åŒ–
        for data in train_dataset + val_dataset + test_dataset:
            data.pos = (data.pos - pos_mean) / pos_std
        
        # DataLoader
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True
        )
        
        return train_loader, val_loader, test_loader
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        model = AdvancedCoordinatePredictor(
            hidden_dim=self.config['hidden_dim'],
            num_layers=self.config['num_layers'],
            model_type=self.config['model_type']
        ).to(self.device)
        
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        return model
    
    def create_optimizer_scheduler(self, model):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.7,
            patience=5
        )
        
        return optimizer, scheduler
    
    def validate(self, model, val_loader):
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        total_rmse = 0
        total_mae = 0
        total_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(self.device)
                pred_pos = model(batch)
                
                rmse = torch.sqrt(F.mse_loss(pred_pos, batch.pos))
                mae = F.l1_loss(pred_pos, batch.pos)
                
                total_rmse += rmse.item()
                total_mae += mae.item()
                total_batches += 1
        
        return total_rmse / total_batches, total_mae / total_batches
    
    def save_checkpoint(self, model, optimizer, epoch, pos_mean, pos_std, val_rmse):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'pos_mean': pos_mean,
            'pos_std': pos_std,
            'val_rmse': val_rmse,
            'config': self.config,
            'train_history': self.train_history
        }
        
        checkpoint_path = self.output_dir / f'checkpoint_epoch_{epoch}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_path = self.output_dir / 'best_model.pth'
        if not best_path.exists() or val_rmse < torch.load(best_path, weights_only=True)['val_rmse']:
            torch.save(checkpoint, best_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯RMSE: {val_rmse:.6f}")
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(15, 5))
        
        # è®­ç»ƒæŸå¤±
        plt.subplot(1, 3, 1)
        plt.plot(self.train_history['train_loss'])
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('MSE Loss')
        plt.yscale('log')
        
        # éªŒè¯RMSE
        plt.subplot(1, 3, 2)
        plt.plot(self.train_history['val_rmse'])
        plt.title('Validation RMSE')
        plt.xlabel('Epoch')
        plt.ylabel('RMSE (Ã…)')
        
        # å­¦ä¹ ç‡
        plt.subplot(1, 3, 3)
        plt.plot(self.train_history['learning_rates'])
        plt.title('Learning Rate')
        plt.xlabel('Epoch')
        plt.ylabel('LR')
        plt.yscale('log')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)
        plt.close()
    
    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        logger.info("=== å¤§è§„æ¨¡3Dåˆ†å­åæ ‡é¢„æµ‹è®­ç»ƒå¼€å§‹ ===")
        start_time = time.time()
        
        # 1. åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset()
        
        # 2. åæ ‡å½’ä¸€åŒ–
        pos_mean, pos_std = self.normalize_coordinates(dataset)
        
        # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = self.create_data_loaders(dataset, pos_mean, pos_std)
        
        # 4. åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # 5. åˆ›å»ºä¼˜åŒ–å™¨
        optimizer, scheduler = self.create_optimizer_scheduler(model)
        
        # 6. è®­ç»ƒå¾ªç¯
        best_val_rmse = float('inf')
        patience_counter = 0
        
        logger.info("å¼€å§‹è®­ç»ƒ...")
        for epoch in range(self.config['max_epochs']):
            # è®­ç»ƒ
            model.train()
            total_loss = 0
            num_batches = 0
            
            for batch_idx, batch in enumerate(train_loader):
                batch = batch.to(self.device)
                optimizer.zero_grad()
                
                pred_pos = model(batch)
                loss = F.mse_loss(pred_pos, batch.pos)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # æ‰“å°è¿›åº¦
                if batch_idx % 100 == 0:
                    logger.info(f"Epoch {epoch+1}/{self.config['max_epochs']}, "
                              f"Batch {batch_idx}/{len(train_loader)}, "
                              f"Loss: {loss.item():.6f}")
            
            avg_train_loss = total_loss / num_batches
            
            # éªŒè¯
            val_rmse, val_mae = self.validate(model, val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_rmse)
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(avg_train_loss)
            self.train_history['val_rmse'].append(val_rmse)
            self.train_history['val_mae'].append(val_mae)
            self.train_history['learning_rates'].append(current_lr)
            
            # æ‰“å°ç»“æœ
            logger.info(f"Epoch {epoch+1:3d} | "
                       f"Train Loss: {avg_train_loss:.6f} | "
                       f"Val RMSE: {val_rmse:.6f} | "
                       f"Val MAE: {val_mae:.6f} | "
                       f"LR: {current_lr:.2e}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config['save_every'] == 0:
                self.save_checkpoint(model, optimizer, epoch + 1, pos_mean, pos_std, val_rmse)
            
            # æ—©åœ
            if val_rmse < best_val_rmse:
                best_val_rmse = val_rmse
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= self.config['patience']:
                logger.info(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®ï¼Œæœ€ä½³éªŒè¯RMSE: {best_val_rmse:.6f}")
                break
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            if (epoch + 1) % 10 == 0:
                self.plot_training_curves()
        
        # æœ€ç»ˆæµ‹è¯•
        logger.info("è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        test_rmse, test_mae = self.validate(model, test_loader)
        logger.info(f"æœ€ç»ˆæµ‹è¯•ç»“æœ - RMSE: {test_rmse:.6f}, MAE: {test_mae:.6f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        results = {
            'best_val_rmse': best_val_rmse,
            'test_rmse': test_rmse,
            'test_mae': test_mae,
            'training_time': time.time() - start_time,
            'config': self.config
        }
        
        with open(self.output_dir / 'training_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        self.plot_training_curves()
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {time.time() - start_time:.2f}ç§’")
        logger.info(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        
        return model, results

def main():
    """ä¸»å‡½æ•°"""
    # å¤§è§„æ¨¡è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®ç›¸å…³
        'data_dir': 'train_4M_pyg',
        'max_samples': None,  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œæˆ–è®¾ç½®å¦‚100000é™åˆ¶æ ·æœ¬æ•°
        
        # æ¨¡å‹ç›¸å…³
        'model_type': 'gat',  # 'gcn', 'gat', 'transformer'
        'hidden_dim': 256,
        'num_layers': 8,
        
        # è®­ç»ƒç›¸å…³
        'batch_size': 32,  # æ ¹æ®GPUå†…å­˜è°ƒæ•´
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'max_epochs': 200,
        'patience': 20,
        
        # ç³»ç»Ÿç›¸å…³
        'num_workers': 8,  # æ•°æ®åŠ è½½å¹¶è¡Œæ•°
        'output_dir': f'large_scale_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'save_every': 10,  # æ¯10è½®ä¿å­˜ä¸€æ¬¡
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LargeScaleTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    model, results = trainer.train()
    
    print("\n" + "="*60)
    print("ğŸ‰ å¤§è§„æ¨¡è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯RMSE: {results['best_val_rmse']:.6f} Ã…")
    print(f"æµ‹è¯•RMSE: {results['test_rmse']:.6f} Ã…")
    print(f"æµ‹è¯•MAE: {results['test_mae']:.6f} Ã…")
    print(f"è®­ç»ƒæ—¶é—´: {results['training_time']:.2f} ç§’")
    print("="*60)

if __name__ == "__main__":
    main()
